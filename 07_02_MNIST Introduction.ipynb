{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### MNIST Introduction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "MNIST는 (Handwritten digits dataset) 손으로 쓴 아라비아 숫자 이미지를 의미한다. <br><br>\n",
    "각 image file은\n",
    "* 28 x 28 \n",
    "* 1 channel gray image\n",
    "* 0 ~ 9 digits<br>\n",
    "의 특징을 가진다<br><br>\n",
    "** torchvision은 유명한 다양한 dataset을 제공하는 package이다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Epoch / Batch size / Iteration<br><br>\n",
    "\n",
    "epoch = one forward pass and one backward pass of all the training examples<br><br>\n",
    "\n",
    "batch size = the number of training examples in one forward/backward pass. The higher the batch size. the more memory space you'll need<br><br>\n",
    "\n",
    "number of iterations = number of passes, each pass using [batch size] number of examples. To be clear, one pass = one forward pass + one backward pass(we do not count the forward pass and backward pass as two different psses.<br><br>\n",
    "\n",
    "Example : If you have 1000 traning examples, and your batch size is 500. then it will take 2 iterations to complete 1 epoch."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torchvision.datasets as dsets\n",
    "import torchvision.transforms as transforms\n",
    "import matplotlib.pyplot as plt\n",
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# cuda(gpu 사용) 만약 없다면 CPU로...\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "\n",
    "# for reproducibility\n",
    "random.seed(777)\n",
    "torch.manual_seed(777)\n",
    "if device == 'cuda':\n",
    "    torch.cuda.manual_seed_all(777)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# parameters\n",
    "training_epochs = 15\n",
    "batch_size = 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# MNIST dataset\n",
    "# torchvision의 MNIST는 4개의 인자를 받는다.\n",
    "# root(위치) train(T->trainset y->testset 호출) transform(적용할 transform 선택), download()\n",
    "mnist_train = dsets.MNIST(root='MNIST_data/',\n",
    "                          train=True,\n",
    "                          transform=transforms.ToTensor(),\n",
    "                          download=True)\n",
    "\n",
    "mnist_test = dsets.MNIST(root='MNIST_data/',\n",
    "                         train=False,\n",
    "                         transform=transforms.ToTensor(),\n",
    "                         download=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# dataset loader\n",
    "# data_loader 함수를 사용하여 data를 불러온다.\n",
    "# DataLoader(무슨 data인가) batch_size(잘라오는 크기),\n",
    "# suffle(불러올 떄 무작위로 불러올 것인가.), drop_last(잘라온 뒤 남는 data는 어떻게 할 것인가.)\n",
    "data_loader = torch.utils.data.DataLoader(dataset=mnist_train,\n",
    "                                          batch_size=batch_size,\n",
    "                                          shuffle=True,\n",
    "                                          drop_last=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# MNIST data image of shape 28 * 28 = 784\n",
    "# Softmax를 사용하기 위해 torch.nn.Linear 사용\n",
    "# MNIST가 28x28이기떄문에 입력은 784, 10은 MNIST의 Label(0~9)\n",
    "linear = torch.nn.Linear(784, 10, bias=True).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define cost/loss & optimizer\n",
    "criterion = torch.nn.CrossEntropyLoss().to(device)    # Softmax is internally computed. (in pytorch)\n",
    "optimizer = torch.optim.SGD(linear.parameters(), lr=0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0001 cost = 0.535468459\n",
      "Epoch: 0002 cost = 0.359274179\n",
      "Epoch: 0003 cost = 0.331187546\n",
      "Epoch: 0004 cost = 0.316578031\n",
      "Epoch: 0005 cost = 0.307158172\n",
      "Epoch: 0006 cost = 0.300180703\n",
      "Epoch: 0007 cost = 0.295130253\n",
      "Epoch: 0008 cost = 0.290851563\n",
      "Epoch: 0009 cost = 0.287417084\n",
      "Epoch: 0010 cost = 0.284379542\n",
      "Epoch: 0011 cost = 0.281825215\n",
      "Epoch: 0012 cost = 0.279800683\n",
      "Epoch: 0013 cost = 0.277808994\n",
      "Epoch: 0014 cost = 0.276154310\n",
      "Epoch: 0015 cost = 0.274440855\n",
      "Learning finished\n"
     ]
    }
   ],
   "source": [
    "for epoch in range(training_epochs):\n",
    "    avg_cost = 0\n",
    "    total_batch = len(data_loader)\n",
    "\n",
    "    for X, Y in data_loader:\n",
    "        # reshape input image into [batch_size by 784]\n",
    "        # label is not one-hot encoded\n",
    "        X = X.view(-1, 28 * 28).to(device)\n",
    "        Y = Y.to(device)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        hypothesis = linear(X)\n",
    "        cost = criterion(hypothesis, Y)\n",
    "        cost.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        avg_cost += cost / total_batch\n",
    "\n",
    "    print('Epoch:', '%04d' % (epoch + 1), 'cost =', '{:.9f}'.format(avg_cost))\n",
    "\n",
    "print('Learning finished')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.8863000273704529\n",
      "Label:  5\n",
      "Prediction:  8\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Visualization\n",
    "# Test the model using test sets\n",
    "with torch.no_grad():\n",
    "    X_test = mnist_test.test_data.view(-1, 28 * 28).float().to(device)\n",
    "    Y_test = mnist_test.test_labels.to(device)\n",
    "\n",
    "    prediction = linear(X_test)\n",
    "    correct_prediction = torch.argmax(prediction, 1) == Y_test\n",
    "    accuracy = correct_prediction.float().mean()\n",
    "    print('Accuracy:', accuracy.item())\n",
    "\n",
    "    # Get one and predict\n",
    "    r = random.randint(0, len(mnist_test) - 1)\n",
    "    X_single_data = mnist_test.test_data[r:r + 1].view(-1, 28 * 28).float().to(device)\n",
    "    Y_single_data = mnist_test.test_labels[r:r + 1].to(device)\n",
    "\n",
    "    print('Label: ', Y_single_data.item())\n",
    "    single_prediction = linear(X_single_data)\n",
    "    print('Prediction: ', torch.argmax(single_prediction, 1).item())\n",
    "\n",
    "    plt.imshow(mnist_test.test_data[r:r + 1].view(28, 28), cmap='Greys', interpolation='nearest')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
